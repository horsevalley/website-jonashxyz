[{"content":"I\u0026rsquo;ve been using this terminal file manager for a few weeks now, and I must say; I absolutely love it! It\u0026rsquo;s written in Rust, so all I/O operations are asynchronous, which means CPU tasks are spread across multiple threads, utilizing available resources. And the best part; it has vim keybindings!\nNot convinced yet? Check out the GitHub page, and scroll down a bit until you reach \u0026ldquo;example.mp4\u0026rdquo;. You\u0026rsquo;ll see a video that demonstrates some of its cool features.\nYou can check out other nice features here, like scrollable preview, bulk rename and integration with tools like fd, rg, ueberzugpp and more.\nInstallation and quick start https://yazi-rs.github.io/docs/installation/\nGood luck!\nLinks: 202405211803\nhttps://github.com/sxyazi/yazi\nhttps://yazi-rs.github.io/\n","permalink":"https://jonash.xyz/posts/yazi---a-blazing-fast-terminal-file-manager/","summary":"I\u0026rsquo;ve been using this terminal file manager for a few weeks now, and I must say; I absolutely love it! It\u0026rsquo;s written in Rust, so all I/O operations are asynchronous, which means CPU tasks are spread across multiple threads, utilizing available resources. And the best part; it has vim keybindings!\nNot convinced yet? Check out the GitHub page, and scroll down a bit until you reach \u0026ldquo;example.mp4\u0026rdquo;. You\u0026rsquo;ll see a video that demonstrates some of its cool features.","title":"Yazi - a Blazing Fast Terminal File Manager written in Rust"},{"content":"Network Prefixes Network prefixes are a fundamental concept in IP networking, particularly in the context of CIDR (Classless Inter-Domain Routing). They define the portion of an IP address that is used for network identification, as opposed to host identification within that network. A network prefix is a part of an IP address that indicates the network to which the IP address belongs. It is represented by a combination of an IP address and a subnet mask or prefix length.\nCIDR Notation CIDR notation expresses an IP address and its associated network prefix in a concise format:\nFormat: IP_address/prefix_length Example: 192.168.1.0/24 In this example, 192.168.1.0 is the network address, and /24 indicates that the first 24 bits of the IP address are used for the network prefix. The remaining bits (32 - 24 = 8 bits) are used for host addresses within that network.\nNetwork Prefix Sizes and Their Reasoning The size of the network prefix determines the number of networks and the number of hosts per network. The prefix length specifies how many bits of the IP address are used for the network portion:\nShorter Prefix (e.g., /8): More bits for the host part, fewer networks, and more hosts per network. Longer Prefix (e.g., /24): More bits for the network part, more networks, and fewer hosts per network. IPv4 Addressing IPv4 addresses are 32 bits long, and the prefix length can range from 0 to 32:\n/8 Prefix:\nNetwork portion: 8 bits Host portion: 24 bits Example: 10.0.0.0/8 Number of networks: 2^8 = 256 Number of hosts per network: 2^24 - 2 = 16,777,214 (subtracting 2 for network and broadcast addresses) /16 Prefix:\nNetwork portion: 16 bits Host portion: 16 bits Example: 192.168.0.0/16 Number of networks: 2^16 = 65,536 Number of hosts per network: 2^16 - 2 = 65,534 /24 Prefix:\nNetwork portion: 24 bits Host portion: 8 bits Example: 192.168.1.0/24 Number of networks: 2^24 = 16,777,216 Number of hosts per network: 2^8 - 2 = 254 Reasoning Behind Prefix Sizes Efficient Use of IP Space:\nPrefix sizes allow networks to be divided into appropriately sized subnets, optimizing the allocation of IP addresses. Organizations can choose a prefix size that matches their network size, reducing waste and ensuring efficient utilization. Hierarchical Network Design:\nPrefix sizes support hierarchical structuring of networks, simplifying routing and management. Larger prefixes (/8, /16) are used for broader network segments, while smaller prefixes (/24, /28) are used for specific subnets. Scalability and Flexibility:\nCIDR and variable prefix lengths enable scalable network design, accommodating both large and small networks. Flexibility in prefix length allows networks to be easily subdivided or aggregated as needed. Routing Efficiency:\nLarger prefixes reduce the number of routing table entries by aggregating multiple networks into a single route. This aggregation improves routing efficiency and performance by minimizing the size of routing tables. Practical Example Consider a company with three departments, each requiring a separate subnet with around 50 devices. Using CIDR, you can allocate:\nSubnet for Department A: 192.168.1.0/26 (64 addresses, 62 usable) Subnet for Department B: 192.168.1.64/26 (64 addresses, 62 usable) Subnet for Department C: 192.168.1.128/26 (64 addresses, 62 usable) Each department gets a subnet that closely matches its needs, optimizing IP address usage.\nSummary Network prefixes are crucial for defining the structure and size of IP networks. The length of the prefix determines the division between the network and host portions of an IP address, impacting the number of available networks and hosts. By using CIDR notation and flexible prefix lengths, network administrators can efficiently allocate IP address space, design scalable networks, and optimize routing performance.\nLinks: 202405171407\nRFC 1918 discusses private IP address allocation, which is crucial for understanding network prefixes in private networks.\nIP Address Guide provides detailed explanations and tools for understanding and calculating IP addresses and subnets.\nNetwork Computing offers a variety of articles and tutorials on networking topics.\nCisco Documentation provides detailed technical guides and white papers on various networking topics, including IP addressing and subnetting.\nJuniper Networks offers comprehensive technical documentation on IP networking and related topics.\nIEEE Xplore is a digital library for research papers and articles on electrical engineering and computer science, including networking topics.\nACM Digital Library provides access to a vast collection of research articles and papers on computer science, including networking.\n","permalink":"https://jonash.xyz/posts/understanding-network-prefixes/","summary":"Network Prefixes Network prefixes are a fundamental concept in IP networking, particularly in the context of CIDR (Classless Inter-Domain Routing). They define the portion of an IP address that is used for network identification, as opposed to host identification within that network. A network prefix is a part of an IP address that indicates the network to which the IP address belongs. It is represented by a combination of an IP address and a subnet mask or prefix length.","title":"Understanding Network Prefixes"},{"content":"Classless Inter-Domain Routing (CIDR) is more efficient than the older classful network design for several key reasons;\n1. Flexible Subnetting and Address Allocation Classful Network Design: IP addresses were divided into fixed classes (A, B, C, D, E). Class A: 16 million addresses (8-bit network prefix). Class B: 65,536 addresses (16-bit network prefix). Class C: 256 addresses (24-bit network prefix). This fixed structure often led to significant wastage of IP addresses. For example, a network needing 300 addresses would require a Class B allocation, wasting most of the 65,536 addresses. CIDR: Allows for arbitrary-length network prefixes, enabling more precise allocation of IP addresses. Example: A CIDR block like 192.168.0.0/22 can allocate 1,024 addresses (covering 192.168.0.0 to 192.168.3.255), which is more efficient than allocating a larger block unnecessarily. This flexibility reduces wastage and allows for better utilization of IP address space. 2. Simplified Routing and Reduced Routing Table Size Classful Network Design Each network class required its entry in routing tables. The rigid class boundaries could lead to numerous routing table entries, especially with many small networks. CIDR Enables route aggregation (supernetting), which combines multiple IP address ranges into a single routing table entry. Example: Multiple networks like 192.168.0.0/24, 192.168.1.0/24, 192.168.2.0/24, and 192.168.3.0/24 can be aggregated into a single CIDR block 192.168.0.0/22. This reduces the number of entries in routing tables, leading to faster and more efficient routing. 3. Improved Internet Scalability Classful Network Design The rapid growth of the Internet led to the exhaustion of available Class B and C addresses. The class-based allocation system struggled to keep up with the expanding number of networks. CIDR Delays IPv4 address exhaustion by allowing more granular allocation of address space. Supports hierarchical IP address allocation, which improves the scalability of the global routing system. Internet Service Providers (ISPs) can allocate IP addresses more efficiently, reducing the need for frequent renumbering and reallocation. 4. Enhanced Network Design Flexibility Classful Network Design Networks were constrained to predefined sizes, making it difficult to design networks that matched organizational needs precisely. Limited flexibility in managing and optimizing network resources. CIDR Provides the ability to create subnets and supernets tailored to specific requirements. Organizations can design networks that closely match their size and growth expectations, optimizing resource usage and management. Summary of Efficiency Gains Address Utilization: CIDR reduces IP address wastage by allowing precise subnetting. Routing Efficiency: CIDR minimizes routing table size through route aggregation, enhancing routing performance. Scalability: CIDR supports the growth of the Internet by enabling more efficient use of the IPv4 address space. Flexibility: CIDR offers greater flexibility in network design, accommodating diverse organizational needs. Practical Example Imagine a company needing IP addresses for 500 hosts. Under the classful system, they would have to use a Class B network (/16), which provides 65,536 addresses, wasting a vast majority. With CIDR, they can allocate a 23-bit network prefix (e.g., 192.168.0.0/23), providing 512 addresses, which is a much more efficient use of IP space.\nBy adopting CIDR, the efficiency of IP address allocation and routing is significantly improved, making it a preferred choice for modern networking.\nLinks: 202405171433\nIP Address Guide provides detailed explanations and tools for understanding and calculating IP addresses and subnets.\nRFC 1519 is the original document that specifies CIDR.\nRFC 1918 discusses private IP address allocation, which is crucial for understanding network prefixes in private networks.\nIETF publishes a wide range of RFCs and documents related to internet standards, including those on IP networking and CIDR.\nClassless Inter-Domain Routing\nNetwork Computing offers a variety of articles and tutorials on networking topics.\nCisco Documentation provides detailed technical guides and white papers on various networking topics, including IP addressing and subnetting.\nJuniper Networks offers comprehensive technical documentation on IP networking and related topics.\nIEEE Xplore is a digital library for research papers and articles on electrical engineering and computer science, including networking topics.\nACM Digital Library provides access to a vast collection of research articles and papers on computer science, including networking.\nBooks \u0026ldquo;Computer Networks\u0026rdquo; by Andrew S. Tanenbaum and David J. Wetherall\nA comprehensive textbook covering a wide range of topics in computer networking, including IP addressing and subnetting. \u0026ldquo;TCP/IP Illustrated, Volume 1: The Protocols\u0026rdquo; by W. Richard Stevens\nA detailed guide to the TCP/IP protocol suite, providing in-depth coverage of IP networking concepts. \u0026ldquo;Internetworking with TCP/IP Volume One\u0026rdquo; by Douglas E. Comer\nAnother authoritative resource that explains the principles of TCP/IP networking, including IP addressing and CIDR. ","permalink":"https://jonash.xyz/posts/why-is-cidr-routing-more-efficient-than-classful-network-design/","summary":"Classless Inter-Domain Routing (CIDR) is more efficient than the older classful network design for several key reasons;\n1. Flexible Subnetting and Address Allocation Classful Network Design: IP addresses were divided into fixed classes (A, B, C, D, E). Class A: 16 million addresses (8-bit network prefix). Class B: 65,536 addresses (16-bit network prefix). Class C: 256 addresses (24-bit network prefix). This fixed structure often led to significant wastage of IP addresses.","title":"Why is CIDR routing more efficient than classful network design?"},{"content":"#Kubernetes #Helm #DevOps\nHelm Helm is the best way to find, share, and use software built for Kubernetes. It helps you manage Kubernetes applications and helps you define, install, and upgrade even the most complex Kubernetes application.\nHelm is a graduated project in the CNCF and is maintained by the Helm community.\nWhat is a helm chart? a Helm chart is a powerful tool that simplifies the deployment, management, and sharing of applications on Kubernetes by packaging all necessary resources and configurations into a single, reusable unit.\nIn other words, a Helm chart is a package that contains all the necessary information and resources to deploy an application or service onto a Kubernetes cluster. It includes a collection of files that describe a related set of Kubernetes resources.\nCharts are easy to create, version, share, and publish — so start using Helm and stop the copy-and-paste.\nHow to install helm https://helm.sh/docs/intro/install/\nExample Structure of a Helm Chart mychart/ Chart.yaml # Chart metadata values.yaml # Default configuration values charts/ # Dependency charts (optional) templates/ # Templates for Kubernetes resources deployment.yaml service.yaml ... README.md # Documentation (optional) Components and functionality of a Helm chart: Chart.yaml: This file contains metadata about the chart, including the name, version, description, and any dependencies on other charts.\nValues.yaml: This file defines the default configuration values for the chart. Users can override these values when they install or upgrade a chart to customize the deployment.\nTemplates: This directory contains a set of templates that generate Kubernetes manifest files. The templates use Go templating syntax and can include variables that are defined in Values.yaml or provided by the user at installation time.\nCharts: This directory (optional) can contain dependencies, which are other charts that this chart depends on.\nFiles: Additional files can be included in the chart for reference, such as README.md or any other documentation.\nHow Helm Charts Work Packaging: Helm charts are packaged into .tgz (tarball) files, which can be distributed and shared. Installation: When you install a chart using the helm install command, Helm takes the templates, substitutes any values, and generates Kubernetes manifests that are then applied to the Kubernetes cluster. Customization: Users can provide their own configuration values via the --values or --set flags to customize the deployment without modifying the chart\u0026rsquo;s source code. Versioning: Charts can be versioned, allowing for consistent deployments and the ability to roll back to previous versions if necessary. What are the benefits of using Helm Charts? Reusability: Helm charts encapsulate Kubernetes resources into reusable packages, making it easy to share and reuse configurations across different environments. Simplified Management: Helm abstracts the complexity of Kubernetes configurations, making it easier to manage and deploy applications. Dependency Management: Helm manages dependencies between different charts, ensuring that all required services are deployed in the correct order. Version Control: Helm charts support versioning, which helps in tracking changes and rolling back to previous versions when needed. Customization: The ability to override default values allows users to customize deployments without altering the original chart. Common Helm commands helm repo add \u0026lt;repo-name\u0026gt; \u0026lt;repo-url\u0026gt;\nAdds a Helm chart repository to your local Helm client. Example: helm repo add stable https://charts.helm.sh/stable helm repo update\nUpdates the local cache of the Helm chart repositories. Example: helm repo update helm search repo \u0026lt;keyword\u0026gt;\nSearches for charts in the added repositories that match the given keyword. Example: helm search repo nginx helm install \u0026lt;release-name\u0026gt; \u0026lt;chart\u0026gt; [flags]\nInstalls a Helm chart to create a new release. Example: helm install my-nginx stable/nginx-ingress helm upgrade \u0026lt;release-name\u0026gt; \u0026lt;chart\u0026gt; [flags]\nUpgrades an existing release to a new version of the chart. Example: helm upgrade my-nginx stable/nginx-ingress helm uninstall \u0026lt;release-name\u0026gt; [flags]\nUninstalls a release from the Kubernetes cluster. Example: helm uninstall my-nginx helm list [flags]\nLists all releases in the current namespace. Example: helm list helm status \u0026lt;release-name\u0026gt;\nDisplays the status of the specified release. Example: helm status my-nginx helm rollback \u0026lt;release-name\u0026gt; \u0026lt;revision\u0026gt;\nRolls back a release to a specific revision. Example: helm rollback my-nginx 1 helm template \u0026lt;chart\u0026gt; [flags]\nGenerates Kubernetes manifest files from a Helm chart without actually installing the chart. Example: helm template stable/nginx-ingress helm show values \u0026lt;chart\u0026gt;\nDisplays the default values for a Helm chart. Example: helm show values stable/nginx-ingress helm get all \u0026lt;release-name\u0026gt;\nRetrieves all information about a specific release, including values, hooks, and manifest files. Example: helm get all my-nginx helm package \u0026lt;chart-path\u0026gt; [flags]\nPackages a Helm chart directory into a .tgz (tarball) file. Example: helm package ./mychart helm lint \u0026lt;chart\u0026gt;\nRuns a series of tests to ensure that a chart follows best practices. Example: helm lint ./mychart helm test \u0026lt;release-name\u0026gt; [flags]\nRuns tests for a release to validate its deployment. Example: helm test my-nginx helm dependency update [flags]\nUpdates the dependencies for a chart based on the Chart.yaml file. Example: helm dependency update ./mychart helm pull \u0026lt;chart\u0026gt; [flags]\nDownloads a chart from a repository and (optionally) decompresses it. Example: helm pull stable/nginx-ingress helm repo list\nLists all the repositories that have been added. Example: helm repo list These commands cover the most common tasks you\u0026rsquo;ll perform with Helm, from managing repositories and searching for charts to installing, upgrading, and maintaining releases. Check out the official documentation page to learn more about Helm.\nGood luck!\nLinks : 202405161033\nhttps://helm.sh/\nhttps://helm.sh/docs/\n","permalink":"https://jonash.xyz/posts/helm---the-package-manager-for-kubernetes/","summary":"#Kubernetes #Helm #DevOps\nHelm Helm is the best way to find, share, and use software built for Kubernetes. It helps you manage Kubernetes applications and helps you define, install, and upgrade even the most complex Kubernetes application.\nHelm is a graduated project in the CNCF and is maintained by the Helm community.\nWhat is a helm chart? a Helm chart is a powerful tool that simplifies the deployment, management, and sharing of applications on Kubernetes by packaging all necessary resources and configurations into a single, reusable unit.","title":"Helm - The package manager for Kubernetes"},{"content":"The relationship between nodes and pods In Kubernetes, the relationship between nodes and pods is central to how applications are deployed and managed. Understanding what nodes and pods are, individually, helps clarify their interaction.\nWhat are Pods? A pod is the smallest deployable unit in Kubernetes and serves as a wrapper for one or more containers. Each pod is designed to run a single instance of a given application or service. It can contain one or multiple containers (usually Docker containers), and these containers within a pod share resources like networking and storage. Containers in the same pod can communicate with each other using localhost, as they share the same network namespace.\nPods encapsulate:\nApplication containers Storage resources A unique network IP Options that govern how the container(s) should run A pod is designed to run a single instance of a given application. If your application requires scaling, you create multiple instances of the same pod, each of which might be identical but isolated from others.\nWhat are Nodes? A node is a worker machine in a Kubernetes cluster. This machine can be either a physical computer or a virtual machine (VM) depending on the environment in which the Kubernetes cluster is deployed. Each node is managed by the master components (such as the scheduler, API server, and controller manager), which make decisions about where to place pods, monitor node and pod status, and respond to changes in the cluster. Nodes have the necessary services to run pods, including the Docker runtime and the Kubelet, which is responsible for maintaining a set of pods as specified by the Kubernetes API.\nEach node in a Kubernetes cluster fulfills one or more of the following roles:\nMaster Node: Runs cluster management tasks. Worker Node: Hosts the Pods that are the components of the application workload. Nodes are responsible for providing the resources (CPU, memory, network, and storage) that pods need to run their applications.\nRelationship between Nodes and Pods Nodes provide the runtime environments for pods. Each node can host multiple pods. Pods are assigned to nodes by the Kubernetes scheduler based on resource availability, policy, and other constraints. Each pod is bound to a node and remains on that node until terminated or deleted. If a node fails, the pods scheduled on that node are scheduled for deletion, and they may be recreated by the control plane on other nodes. This relationship enables Kubernetes to manage large-scale, distributed environments efficiently, ensuring that applications are reliably available and can be scaled as needed across the various nodes in the cluster.\nLinks: 202405142258\nKubernetes documentation home: https://kubernetes.io/docs/home/\n","permalink":"https://jonash.xyz/posts/kubernetes---understanding-the-relationship-between-nodes-and-pods/","summary":"The relationship between nodes and pods In Kubernetes, the relationship between nodes and pods is central to how applications are deployed and managed. Understanding what nodes and pods are, individually, helps clarify their interaction.\nWhat are Pods? A pod is the smallest deployable unit in Kubernetes and serves as a wrapper for one or more containers. Each pod is designed to run a single instance of a given application or service.","title":"Kubernetes - understanding the relationship between nodes and pods"},{"content":"Today I learned that if you have Inkscape installed on your machine, you can convert .SVG files into .PDF, using this simple command:\ninkscape input.svg --export-filename=output.pdf Replace input.svg with the name of your SVG file and output.pdf with the desired output PDF file name. Example use case Say you have developed a drawing or mindmap in Excalidraw. You can export this to SVG and then run the command to convert it to PDF. Very cool!\nLinks : 202403241251\n","permalink":"https://jonash.xyz/posts/convert-svg-to-pdf-using-inkscape/","summary":"Today I learned that if you have Inkscape installed on your machine, you can convert .SVG files into .PDF, using this simple command:\ninkscape input.svg --export-filename=output.pdf Replace input.svg with the name of your SVG file and output.pdf with the desired output PDF file name. Example use case Say you have developed a drawing or mindmap in Excalidraw. You can export this to SVG and then run the command to convert it to PDF.","title":"Convert SVG to PDF from your command line using Inkscape"},{"content":"obsidian-sync-github Why write this script? I\u0026rsquo;m using Arch Linux and the Obsidian electron AppImage from AUR. It works great, but for some reason the obsidian git community plugin makes the app super sluggish. I removed the plugin and wrote my own bash script instead.\nHow the script works Auto-pushes Obsidian notes to GitHub via a cronjob every 30 minutes crontab -e -\u0026gt; */30 * * * * /home/jonash/.local/bin/obsidian-sync-github Sends a notification via dunstify and your e-mail address of choice #!/bin/bash # Navigate to your repository directory cd ~/obsidian/ # Add all changes to git git add . # Commit the changes with a current timestamp git commit -m \u0026#34;Automated commit on $(date)\u0026#34; # Push the changes git push origin main # Send notification after the push is sent if git push origin main; then # Check if dunstify is available (dunst\u0026#39;s notification tool) if command -v dunstify \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then # Send a notification dunstify \u0026#34;Successfully pushed obsidian notes to GitHub!\u0026#34; else echo \u0026#34;dunstify not found, cannot send notification\u0026#34; fi # Email details recipient=\u0026#34;jonash@jonash.xyz\u0026#34; subject=\u0026#34;Subject: Obsidian Notes Push Successful\u0026#34; body=\u0026#34;Your Obsidian notes have been successfully pushed to GitHub on $(date)\u0026#34; # Send the email echo -e \u0026#34;$subject\\n\\n$body\u0026#34; | msmtp \u0026#34;$recipient\u0026#34; else echo \u0026#34;git push failed\u0026#34; fi ","permalink":"https://jonash.xyz/posts/obsidian-sync-github-script/","summary":"obsidian-sync-github Why write this script? I\u0026rsquo;m using Arch Linux and the Obsidian electron AppImage from AUR. It works great, but for some reason the obsidian git community plugin makes the app super sluggish. I removed the plugin and wrote my own bash script instead.\nHow the script works Auto-pushes Obsidian notes to GitHub via a cronjob every 30 minutes crontab -e -\u0026gt; */30 * * * * /home/jonash/.local/bin/obsidian-sync-github Sends a notification via dunstify and your e-mail address of choice #!","title":"Obsidian Sync Github Script"},{"content":"What is HUGO? HUGO is a framework, designed to convert code and content into static websites. It is written in Go, making it exceptionally fast at compiling websites, even those with thousands of pages. Here\u0026rsquo;s how it works:\nFast and Efficient: HUGO takes your content, which you can write in Markdown (a lightweight markup language), and your templates, which define the structure and design of your site, and combines them to generate a complete, static website. These websites are made up of prebuilt HTML files, making them very fast to load and easy to host.\nNo Database Required: Unlike dynamic site generators that require databases to store your site\u0026rsquo;s content, HUGO works with files only. This means your website\u0026rsquo;s content and layout are defined through text files, templates, and configurations without needing a database. This simplicity leads to increased security and ease of maintenance.\nCustomizable: HUGO is highly customizable, allowing you to create websites that range from blogs and portfolios to documentation and company websites, all while providing a wide range of themes and plugins to enhance functionality and appearance.\nLive Reloading: It offers a live reload feature, which means you can see the changes you make in real-time as you develop your site. This makes the development process more intuitive and faster.\nStatic Site Benefits: Since HUGO generates static websites, these sites are inherently secure, fast, and reliable. They can be hosted on any web server or services like GitHub Pages, Netlify, and Vercel, often with little to no cost for hosting.\nIn essence, HUGO is a tool that helps developers and content creators build websites quickly and efficiently, without worrying about databases or server-side scripting, making it an excellent choice for projects where speed, security, and simplicity are prioritized.\nStep 1: Install HUGO Windows: Use Chocolatey: choco install hugo -confirm macOS: Use Homebrew: brew install hugo Linux: Use your distro\u0026rsquo;s package manager, for example, sudo apt-get install hugo for Ubuntu. Step 2: Create a New Site Open a terminal or command prompt. Navigate to the directory where you want your site. Run hugo new site mywebsite -f \u0026quot;yaml\u0026quot; - replacing mywebsite with your desired site name. Step 3: Add a Theme Find a theme you like from HUGO Themes. Clone the theme into your project: e.g. git clone https://github.com/reorx/hugo-PaperModX themes/PaperModX --depth=1 Add the theme to your site\u0026rsquo;s config file: echo 'theme : nameOfYourTheme' \u0026gt;\u0026gt; config.yml e.g. echo 'theme: PaperModX' \u0026gt;\u0026gt; config.yml you can remove the hugo.toml file, since we are using yaml config instead Step 4: Add Content cd into the archetypes folder and replace the contents of defaults.md with the following code: --- title : \u0026#39;{{ replace .File.ContentBaseName \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#39; date : {{ .Date }} draft : false --- Create a new post: hugo new posts/my-first-post.md Open the file in a text editor, and you\u0026rsquo;ll see some front matter at the top. Below this, add your content It\u0026rsquo;s important that you write your content in markdown format Step 5: Start the HUGO Server Run hugo server in your site directory. Open a web browser and go to http://localhost:1313 to see your site. Step 6: Build Your Site When ready to publish, run hugo to build your static site. The output will be in the public directory, ready to be deployed to your hosting provider. I have added an alias, to my .zshrc file, to update my website after building with the hugo command, with the help of rsync: update_website=\u0026#34;rsync -vrP --delete-after /home/jonash/dotfiles/website/website/jonashxyz/public/ root@jonash.xyz:/var/www/jonashxyz/\u0026#34; \\ I got the alias from Luke Smith Done Congratulations! You\u0026rsquo;ve just created a website with HUGO. For more detailed instructions, visit the official HUGO documentation.\nAlso, read the documentation of your chosen theme, e.g. PaperModX.\nEnjoy!\nLinks:\n202403081313\n","permalink":"https://jonash.xyz/posts/generate-a-website-with-hugo/","summary":"What is HUGO? HUGO is a framework, designed to convert code and content into static websites. It is written in Go, making it exceptionally fast at compiling websites, even those with thousands of pages. Here\u0026rsquo;s how it works:\nFast and Efficient: HUGO takes your content, which you can write in Markdown (a lightweight markup language), and your templates, which define the structure and design of your site, and combines them to generate a complete, static website.","title":"Create Your Own Website With HUGO"},{"content":"Unshackle Your Digital Life Get a Google Pixel phone, so you can install GrapheneOS. I recommend Google Pixel 6a. Very cheap and very good. Get rid of all the apps you don\u0026rsquo;t need Sync your data with SyncThing Create your own NextCloud server Use VimWiki/Obsidian + vim/neovim to take notes in markdown, and take full ownership over your own data Use Linux on your personal computer(s) Use pass to store your accounts/passwords Don\u0026rsquo;t let proprietary software take your money and own YOUR data Never look back ","permalink":"https://jonash.xyz/posts/how-to-live-an-unshackled-digital-life/","summary":"Unshackle Your Digital Life Get a Google Pixel phone, so you can install GrapheneOS. I recommend Google Pixel 6a. Very cheap and very good. Get rid of all the apps you don\u0026rsquo;t need Sync your data with SyncThing Create your own NextCloud server Use VimWiki/Obsidian + vim/neovim to take notes in markdown, and take full ownership over your own data Use Linux on your personal computer(s) Use pass to store your accounts/passwords Don\u0026rsquo;t let proprietary software take your money and own YOUR data Never look back ","title":"Unshackle Your Digital Life"},{"content":"First of all, what is GRUB? GRUB is a boot loader, which lets you boot up your system. The abbreviation stands for GNU GRand Unified Bootloader, and is a boot loader package from the GNU Project. GRUB is the primary bootloader for many Linux distributions, allowing users to choose from multiple operating systems or different versions of the same operating system at boot time. It\u0026rsquo;s designed to be highly configurable and supports a wide range of operating systems, not just Linux.\nWhy make this guide? The reason I made this simple guide is so you know what to do when you get to Arch Linux installation guide, chapter 3.8.\nGRUB features include: The ability to boot multiple operating systems, including Linux, Windows, and macOS. Support for reading data from various file systems, allowing it to load boot files from these file systems directly. A flexible and powerful command-line interface that can be used to troubleshoot boot issues. The capability to load operating systems from a network, which is particularly useful in managed IT environments. A customizable menu interface that can display graphics and themes, allowing for a visually appealing boot menu. GRUB has evolved over time, with GRUB 2 being the successor to the original GRUB (referred to as GRUB Legacy). GRUB 2 offers improved modularity, portability, and features compared to its predecessor.\nInstalling GRUB EFI in Arch Linux First off, we install some packages:\nsudo pacman -S vim networkmanager grub efibootmgr dosfstools os-prober mtools (Networkmanager is optional, but highly recommended)\nEnableNetworkManager systemctl enable NetworkManager Mount EFI-partition mount --mkdir /dev/drive /boot/EFI Install GRUB grub-install --target=x86_64-efi --bootloader-id=grub_uefi --efi-directory=/boot/EFI --no-nvram --removable --recheck Make grub config file grub-mkconfig -o /boot/grub/grub.cfg Reboot Exit the chroot environment by typing exit or pressing Ctrl+d. Optionally manually unmount all the partitions with umount -R /mnt: this allows noticing any \u0026ldquo;busy\u0026rdquo; partitions, and finding the cause Finally, restart the machine by typing reboot: any partitions still mounted will be automatically unmounted by systemd. Remember to remove the installation medium and then login into the new system with the root account ","permalink":"https://jonash.xyz/posts/grub-bootloader/","summary":"First of all, what is GRUB? GRUB is a boot loader, which lets you boot up your system. The abbreviation stands for GNU GRand Unified Bootloader, and is a boot loader package from the GNU Project. GRUB is the primary bootloader for many Linux distributions, allowing users to choose from multiple operating systems or different versions of the same operating system at boot time. It\u0026rsquo;s designed to be highly configurable and supports a wide range of operating systems, not just Linux.","title":"GRUB Bootloader"},{"content":"These modern Linux tools might enhance your workflow fd is a simple and fast file search tool, enhancing the Unix find command. exa is a modern replacement for ls, enhancing file listing with better defaults. dog is a modern, feature-rich DNS client for the command-line. ncdu is a console disk usage analyzer for quick space management. bat is a cat clone with syntax highlighting and Git integration for the command-line. sd is a simple and intuitive find-and-replace CLI tool, aiming to improve upon sed. dust visualizes disk usage with an emphasis on clarity, acting as a more intuitive du. xh is a friendly and fast HTTP client for the terminal, inspired by curl and HTTPie. dufis a modern disk usage utility for the command-line with an intuitive interface. ","permalink":"https://jonash.xyz/posts/modern-linux-tools/","summary":"These modern Linux tools might enhance your workflow fd is a simple and fast file search tool, enhancing the Unix find command. exa is a modern replacement for ls, enhancing file listing with better defaults. dog is a modern, feature-rich DNS client for the command-line. ncdu is a console disk usage analyzer for quick space management. bat is a cat clone with syntax highlighting and Git integration for the command-line. sd is a simple and intuitive find-and-replace CLI tool, aiming to improve upon sed.","title":"Modern Linux Tools"},{"content":"Demystifying Cloud Engineering Cloud engineering is the application of engineering disciplines to cloud computing. It brings a systematic approach to the high-level concerns of commercialization, standardization, and governance in conceiving, developing, operating, and maintaining cloud computing systems. It is a multidisciplinary method that encompasses contributions from diverse areas such as systems engineering, software engineering, web engineering, and database engineering.\nKey Components of Cloud Engineering Cloud Infrastructure: Design and management of the physical and virtual resources necessary for cloud computing.\nCloud Software Development: Building, deploying, and managing software applications that are hosted on cloud platforms.\nCloud Security: Protecting cloud-based systems, data, and infrastructure from cybersecurity threats.\nDevOps in the Cloud: Integrating development and operations teams to improve collaboration and productivity by automating infrastructure, workflows, and continuously measuring application performance.\nBenefits of Cloud Engineering Scalability: Easy to scale computing resources up or down based on demand, ensuring flexibility and cost-effectiveness.\nCost Efficiency: Reduces the need for significant upfront investment in IT infrastructure and decreases the cost of IT management and maintenance.\nInnovation: Facilitates rapid development, testing, and deployment of applications, allowing for faster innovation and time to market.\nReliability: Offers reliable services with data backup, disaster recovery, and redundancy features to ensure continuous operation.\nBest Practices in Cloud Engineering Emphasize Security: Implement robust security measures, including identity and access management, encryption, and security protocols.\nOpt for Multi-Cloud and Hybrid Cloud Strategies: Utilize multiple cloud service providers or a combination of public and private clouds to avoid vendor lock-in and enhance business continuity.\nMonitor and Manage Performance: Continuously monitor cloud resources and applications to optimize performance, reduce costs, and improve customer satisfaction.\nAdopt Automation: Leverage automation for provisioning, deployment, scaling, and management tasks to increase efficiency and reduce human errors.\nConclusion Cloud engineering is a vital field in today\u0026rsquo;s digital landscape, offering scalable, flexible, and cost-effective solutions for businesses. By understanding and applying the principles of cloud engineering, organizations can leverage the full potential of cloud computing to innovate, grow, and stay competitive in the rapidly evolving technology market.\n","permalink":"https://jonash.xyz/posts/cloud-engineering/","summary":"Demystifying Cloud Engineering Cloud engineering is the application of engineering disciplines to cloud computing. It brings a systematic approach to the high-level concerns of commercialization, standardization, and governance in conceiving, developing, operating, and maintaining cloud computing systems. It is a multidisciplinary method that encompasses contributions from diverse areas such as systems engineering, software engineering, web engineering, and database engineering.\nKey Components of Cloud Engineering Cloud Infrastructure: Design and management of the physical and virtual resources necessary for cloud computing.","title":"Cloud Engineering"},{"content":"From darkness to light In winter\u0026rsquo;s clutch, the world lay still,\nA canvas painted in shades of chill.\nThe trees stood bare, the skies so gray,\nHope seemed far, in disarray.\nBut time moves on, as whispers in the wind,\nHint at change, where despair had been pinned.\nThe snow melts away, revealing the ground,\nA symbol of life, soon to be found.\nSpring arrives with a gentle embrace,\nAwakening the earth with its grace.\nFlowers bloom, painting the world anew,\nIn vibrant hues of every hue.\nThe sun breaks through the lingering gloom,\nFilling hearts with warmth, dispelling doom.\nBirds sing songs of joy, so sweet and clear,\nProclaiming the news that happiness is near.\nThis transition, from dark to light,\nReminds us all of the perpetual fight.\nFor after every hardship, there comes ease,\nA cycle of life, meant to appease.\nSo let us welcome spring, with open arms,\nAnd find solace in its charms.\nFor it brings hope, and a fresh start,\nHealing the winter\u0026rsquo;s wounds in our heart.\n","permalink":"https://jonash.xyz/posts/from-darkness-to-light/","summary":"From darkness to light In winter\u0026rsquo;s clutch, the world lay still,\nA canvas painted in shades of chill.\nThe trees stood bare, the skies so gray,\nHope seemed far, in disarray.\nBut time moves on, as whispers in the wind,\nHint at change, where despair had been pinned.\nThe snow melts away, revealing the ground,\nA symbol of life, soon to be found.\nSpring arrives with a gentle embrace,\nAwakening the earth with its grace.","title":"From darkness to light"},{"content":"Understanding DevOps: A Brief Overview DevOps, a compound of development (Dev) and operations (Ops), is a software development and delivery approach that emphasizes communication, collaboration, integration, and automation among software developers and IT operations teams. The goal of DevOps is to improve and speed up the delivery of software applications and services. By fostering a culture where building, testing, and releasing software can happen rapidly, frequently, and more reliably, DevOps has become a key practice in the software industry.\nKey Principles of DevOps Continuous Integration (CI): Developers merge their changes back to the main branch as often as possible. Automated tests run with these integrations to catch bugs quickly.\nContinuous Delivery (CD): This practice involves automatically deploying all code changes to a testing or staging environment after the build stage. It ensures that the software can be reliably released at any time.\nAutomated Testing: Automation in testing reduces manual workload and speeds up the process of software development and deployment.\nInfrastructure as Code (IaC): Managing and provisioning infrastructure through code instead of through manual processes, improving operational efficiency and cloud management.\nMonitoring and Logging: Keeping track of the performance of applications and infrastructure to quickly respond to issues.\nCollaboration and Communication: Encouraging open communication and collaboration within and between teams to enhance the development process.\nBenefits of DevOps Faster Time to Market: Shortened development cycles lead to faster innovation and a quicker time to market.\nImproved Collaboration: Breaking down silos between teams enhances collaboration and efficiency.\nIncreased Efficiency: Automation and streamlined workflows increase the efficiency of the development and deployment processes.\nEnhanced Quality: Continuous integration and delivery ensure that quality is maintained through frequent code updates and testing.\nHigher Customer Satisfaction: Rapid deliveries of updates and new features lead to higher customer satisfaction.\nConclusion DevOps is not just a set of practices but a culture that needs to be adopted for its full potential to be realized. By embracing DevOps, organizations can enhance their ability to deliver applications and services at high velocity, thereby outpacing their competitors in today\u0026rsquo;s fast-paced digital world.\n","permalink":"https://jonash.xyz/posts/what-is-devops/","summary":"Understanding DevOps: A Brief Overview DevOps, a compound of development (Dev) and operations (Ops), is a software development and delivery approach that emphasizes communication, collaboration, integration, and automation among software developers and IT operations teams. The goal of DevOps is to improve and speed up the delivery of software applications and services. By fostering a culture where building, testing, and releasing software can happen rapidly, frequently, and more reliably, DevOps has become a key practice in the software industry.","title":"What Is Devops"},{"content":"","permalink":"https://jonash.xyz/articles/","summary":"articles","title":"Articles"},{"content":"You can get in touch with me in the following ways:\nBy sending me an e-mail at hestdahl@gmail.com\nBy sending me a direct message on LinkedIn: https://www.linkedin.com/in/jonas-hestdahl-b5657052/\nSend me a direct message at https://x.com/jonashestdahl (formerly Twitter).\nThanks for reaching out!\n","permalink":"https://jonash.xyz/contact/","summary":"contact","title":"Contact me"}]